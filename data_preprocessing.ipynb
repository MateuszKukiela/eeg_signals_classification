{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from contextlib import nullcontext\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(l_freq=1, h_freq=45, path_in='./data', output = './processed_data/new/', data_type = 'clean', functions=[], use_filter=True, use_baseline=True, filter_length='auto', n_components=2, verbose=False):\n",
    "    X_test_all = np.zeros((0, 19, 306))\n",
    "    X_train_all = np.zeros((0, 19, 306))\n",
    "    X_valid_all = np.zeros((0, 19, 306))\n",
    "\n",
    "    y_test_all = np.zeros((0))\n",
    "    y_train_all = np.zeros((0))\n",
    "    y_valid_all = np.zeros((0))\n",
    "    \n",
    "    files = os.listdir(path_in)\n",
    "    files = list(filter(lambda x: (x[-4:] == '.set'), files))\n",
    "    files = set(map(lambda x: x[:3], files))\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        word_file = path_in + '/' + filename + '_exp1.set'\n",
    "        pseudo_file = path_in + '/' + filename + '_exp2.set'\n",
    "        print(f'Importing {filename}')\n",
    "        raw_word = mne.io.read_epochs_eeglab(word_file, verbose=False)\n",
    "        raw_pseudo = mne.io.read_epochs_eeglab(pseudo_file, verbose=False)\n",
    "        event_ids = {'APos': 1,'RPos': 2,'ONeu': 3,'ANeu': 4,'ONeg': 5,'OPos': 6,'RNeg': 7,'RNeu': 8,'ANeg': 9,'Pseudo': 10}\n",
    "        raw_pseudo.event_id = event_ids\n",
    "        raw_word.event_id = event_ids\n",
    "        raw_pseudo.events[:,2] = 10\n",
    "\n",
    "        y_word = np.ones(len(raw_word))\n",
    "        y_pseudo = np.zeros(len(raw_pseudo))\n",
    "\n",
    "        X = mne.concatenate_epochs([raw_word, raw_pseudo])\n",
    "        y = np.concatenate([y_word, y_pseudo])\n",
    "\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "        X_test, X_valid, y_test, y_valid = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "        data = [X_train, X_test, X_valid]\n",
    "\n",
    "        for i, _ in enumerate(data):\n",
    "#             print(f'Processing {filename} part {i}')\n",
    "            data[i] =  mne.concatenate_epochs(data[i])\n",
    "            if use_baseline:\n",
    "                data[i] = data[i].apply_baseline(baseline=(None,0), verbose=False)\n",
    "            if use_filter:\n",
    "                data[i] = data[i].filter(l_freq=l_freq, h_freq=h_freq, verbose=False, filter_length=filter_length)\n",
    "\n",
    "            for function in functions:\n",
    "                data[i] = function(data[i], i, n_components=n_components, verbose=verbose)\n",
    "                 \n",
    "            data[i] = data[i].get_data()\n",
    "            \n",
    "        X_train, X_test, X_valid = data\n",
    "\n",
    "        X_train_all = np.concatenate((X_train_all, X_train), axis = 0)\n",
    "        X_test_all = np.concatenate((X_test_all, X_test), axis = 0)\n",
    "        X_valid_all = np.concatenate((X_valid_all, X_valid), axis = 0)\n",
    "\n",
    "        y_train_all = np.concatenate((y_train_all, y_train), axis = 0)\n",
    "        y_test_all = np.concatenate((y_test_all, y_test), axis = 0)\n",
    "        y_valid_all = np.concatenate((y_valid_all, y_valid), axis = 0)\n",
    "        \n",
    "    output = f'{output}/{data_type}/'\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "        \n",
    "    np.save(output + f'X_test.npy', X_test_all)\n",
    "    np.save(output + f'y_test.npy', y_test_all)\n",
    "    np.save(output + f'X_train.npy', X_train_all)\n",
    "    np.save(output + f'y_train.npy', y_train_all)\n",
    "    np.save(output + f'X_valid.npy', X_valid_all)\n",
    "    np.save(output + f'y_valid.npy', y_valid_all)\n",
    "        \n",
    "        \n",
    "def xDawn_denosing(data, i, n_components=2, verbose=False, **kwargs):\n",
    "    if not verbose:\n",
    "        cm = HiddenPrints()\n",
    "    else:\n",
    "        cm = nullcontext()\n",
    "    with cm:\n",
    "        data.events = mne.merge_events(data.events, [1,2,3,4,5,6,7,8,9], 1, replace_events=True)\n",
    "        data.event_id = {'Word': 1, 'Pseudo': 10}\n",
    "        if i == 0:\n",
    "            temp_syg = data.get_data()\n",
    "            temp_syg = temp_syg.reshape([temp_syg.shape[1],temp_syg.shape[0]*temp_syg.shape[2]]) #ERROR\n",
    "            signal_cov = mne.compute_raw_covariance(mne.io.RawArray(temp_syg, data[0].info), verbose=verbose)\n",
    "            global xd\n",
    "            xd = mne.preprocessing.Xdawn(n_components=n_components, signal_cov=signal_cov)\n",
    "            xd.fit(data)\n",
    "        epochs_denoised = xd.apply(data)\n",
    "        data = next(iter(epochs_denoised.values())) #Potencjal wywyolany 200ms gorka\n",
    "    return data\n",
    "    \n",
    "\n",
    "def standardization(data, i, **kwargs):\n",
    "    data_temp = data.get_data()\n",
    "    mean = np.mean(data_temp, axis=2).reshape(data_temp.shape[0], data_temp.shape[1], 1)\n",
    "    std = np.std(data_temp, axis=(1, 2)).reshape(data_temp.shape[0], 1, 1)\n",
    "    data._data = (data_temp - mean)/std\n",
    "    return data\n",
    "\n",
    "\n",
    "def standardizatin_common(data, i, **kwargs):\n",
    "    data_temp = data.get_data()\n",
    "    if i == 0:\n",
    "        global mean\n",
    "        mean = np.mean(np.mean(data_temp, axis=2).reshape(data_temp.shape[0], data_temp.shape[1], 1),axis=0)\n",
    "        global std\n",
    "        std = np.mean(np.mean(np.std(data_temp, axis=(1, 2)).reshape(data_temp.shape[0], 1, 1)))\n",
    "    data._data = (data_temp - mean)/std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing B01\n",
      "Importing B02\n",
      "Importing B03\n",
      "Importing B04\n",
      "Importing B05\n",
      "Importing B06\n",
      "Importing B07\n",
      "Importing B08\n",
      "Importing B09\n",
      "Importing B10\n",
      "Importing B11\n",
      "Importing B15\n",
      "Importing B17\n",
      "Importing B18\n",
      "Importing B20\n",
      "Importing B21\n",
      "Importing B22\n",
      "Importing B24\n",
      "Importing B26\n",
      "Importing B27\n",
      "Importing B28\n",
      "Importing B29\n",
      "Importing B30\n",
      "Importing B31\n",
      "Importing B32\n",
      "Importing B33\n",
      "Importing B34\n",
      "Importing B35\n",
      "Importing B36\n",
      "Importing B37\n"
     ]
    }
   ],
   "source": [
    "data_processing(data_type = 'clean', functions=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing xDawn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing B01\n",
      "Importing B02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/pymatreader.py\u001b[0m in \u001b[0;36mread_mat\u001b[0;34m(filename, variable_names, ignore_fields, uint16_codec)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# avoid open file warnings on error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             raw_data = scipy.io.loadmat(fid, struct_as_record=True,\n\u001b[0m\u001b[1;32m     83\u001b[0m                                         \u001b[0msqueeze_me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/lib/python3.8/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/lib/python3.8/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please use HDF reader for matlab v7.3 files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Please use HDF reader for matlab v7.3 files",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3fe21aaf9ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'xDawn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxDawn_denosing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c6040f2b1fbd>\u001b[0m in \u001b[0;36mdata_processing\u001b[0;34m(l_freq, h_freq, path_in, output, data_type, functions, use_filter, use_baseline, filter_length, n_components, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Importing {filename}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mraw_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_epochs_eeglab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mraw_pseudo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_epochs_eeglab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpseudo_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mevent_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'APos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RPos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ONeu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ANeu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ONeg'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'OPos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RNeg'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RNeu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ANeg'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Pseudo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mraw_pseudo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/io/eeglab/eeglab.py\u001b[0m in \u001b[0;36mread_epochs_eeglab\u001b[0;34m(input_fname, events, event_id, eog, verbose, uint16_codec)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.11\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \"\"\"\n\u001b[0;32m--> 278\u001b[0;31m     epochs = EpochsEEGLAB(input_fname=input_fname, events=events, eog=eog,\n\u001b[0m\u001b[1;32m    279\u001b[0m                           \u001b[0mevent_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                           uint16_codec=uint16_codec)\n",
      "\u001b[0;32m<decorator-gen-198>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_fname, events, event_id, tmin, baseline, reject, flat, reject_tmin, reject_tmax, eog, verbose, uint16_codec)\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/io/eeglab/eeglab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_fname, events, event_id, tmin, baseline, reject, flat, reject_tmin, reject_tmax, eog, verbose, uint16_codec)\u001b[0m\n\u001b[1;32m    449\u001b[0m                  \u001b[0mreject_tmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                  uint16_codec=None):  # noqa: D102\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0meeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_load_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muint16_codec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         if not ((events is None and event_id is None) or\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/io/eeglab/eeglab.py\u001b[0m in \u001b[0;36m_check_load_mat\u001b[0;34m(fname, uint16_codec)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m\"\"\"Check if the mat struct contains 'EEG'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpymatreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0meeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muint16_codec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muint16_codec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'ALLEEG'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         raise NotImplementedError(\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/pymatreader.py\u001b[0m in \u001b[0;36mread_mat\u001b[0;34m(filename, variable_names, ignore_fields, uint16_codec)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mh5py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_h5py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             data = _hdf5todict(hdf5_file, variable_names=variable_names,\n\u001b[0m\u001b[1;32m     92\u001b[0m                                ignore_fields=ignore_fields)\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_hdf5todict\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         return _handle_hdf5_group(hdf5_object, variable_names=variable_names,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                   ignore_fields=ignore_fields)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_handle_hdf5_group\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         return_dict[key] = _hdf5todict(hdf5_object[key],\n\u001b[0m\u001b[1;32m    102\u001b[0m                                        \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                        ignore_fields=ignore_fields)\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_hdf5todict\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         return _handle_hdf5_group(hdf5_object, variable_names=variable_names,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                   ignore_fields=ignore_fields)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_handle_hdf5_group\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         return_dict[key] = _hdf5todict(hdf5_object[key],\n\u001b[0m\u001b[1;32m    102\u001b[0m                                        \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                        ignore_fields=ignore_fields)\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_hdf5todict\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         return _handle_hdf5_group(hdf5_object, variable_names=variable_names,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                   ignore_fields=ignore_fields)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_handle_hdf5_group\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         return_dict[key] = _hdf5todict(hdf5_object[key],\n\u001b[0m\u001b[1;32m    102\u001b[0m                                        \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                        ignore_fields=ignore_fields)\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_hdf5todict\u001b[0;34m(hdf5_object, variable_names, ignore_fields)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_handle_hdf5_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_hdf5todict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhdf5_object\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m_handle_hdf5_dataset\u001b[0;34m(hdf5_object)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcur_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmatlab_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cell'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/src/mne/mne/externals/pymatreader/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhdf5_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcur_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmatlab_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cell'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/media/ukulele/54044af1-9811-4792-8fa9-3e2d94310405/bachelor_thesis/env/lib/python3.8/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdereference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5r.pyx\u001b[0m in \u001b[0;36mh5py.h5r.dereference\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_processing(data_type = 'xDawn', functions=[xDawn_denosing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing standardization common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing B01\n",
      "Importing B02\n",
      "Importing B03\n",
      "Importing B04\n",
      "Importing B05\n",
      "Importing B06\n",
      "Importing B07\n",
      "Importing B08\n",
      "Importing B09\n",
      "Importing B10\n",
      "Importing B11\n",
      "Importing B15\n",
      "Importing B17\n",
      "Importing B18\n",
      "Importing B20\n",
      "Importing B21\n",
      "Importing B22\n",
      "Importing B24\n",
      "Importing B26\n",
      "Importing B27\n",
      "Importing B28\n",
      "Importing B29\n",
      "Importing B30\n",
      "Importing B31\n",
      "Importing B32\n",
      "Importing B33\n",
      "Importing B34\n",
      "Importing B35\n",
      "Importing B36\n",
      "Importing B37\n"
     ]
    }
   ],
   "source": [
    "data_processing(data_type = 'standardization_common', functions=[standardizatin_common])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_participants(l_freq=1, h_freq=45, path_in='./data', output = './processed_data/', data_type = 'clean', functions=[], use_filter=True, use_baseline=True, filter_length='auto', n_components=2, verbose=False):\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    \n",
    "    files = os.listdir(path_in)\n",
    "    files = list(filter(lambda x: (x[-4:] == '.set'), files))\n",
    "    files = set(map(lambda x: x[:3], files))\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        word_file = path_in + '/' + filename + '_exp1.set'\n",
    "        pseudo_file = path_in + '/' + filename + '_exp2.set'\n",
    "        print(f'Importing {filename}')\n",
    "        raw_word = mne.io.read_epochs_eeglab(word_file, verbose=False)\n",
    "        raw_pseudo = mne.io.read_epochs_eeglab(pseudo_file, verbose=False)\n",
    "        event_ids = {'APos': 1,'RPos': 2,'ONeu': 3,'ANeu': 4,'ONeg': 5,'OPos': 6,'RNeg': 7,'RNeu': 8,'ANeg': 9,'Pseudo': 10}\n",
    "        raw_pseudo.event_id = event_ids\n",
    "        raw_word.event_id = event_ids\n",
    "        raw_pseudo.events[:,2] = 10\n",
    "\n",
    "        y_word = np.ones(len(raw_word))\n",
    "        y_pseudo = np.zeros(len(raw_pseudo))\n",
    "\n",
    "        X = mne.concatenate_epochs([raw_word, raw_pseudo])\n",
    "        y = np.concatenate([y_word, y_pseudo])\n",
    "        \n",
    "        if use_baseline:\n",
    "            X = X.apply_baseline(baseline=(None,0), verbose=False)\n",
    "        if use_filter:\n",
    "            X = X.filter(l_freq=l_freq, h_freq=h_freq, verbose=False, filter_length=filter_length)\n",
    "\n",
    "        for function in functions:\n",
    "            X = function(X, n_components=n_components, verbose=verbose)\n",
    "\n",
    "        X = X.get_data()\n",
    "            \n",
    "        X_all.append(X)\n",
    "        y_all.append(y)\n",
    "        \n",
    "    output = f'{output}/{data_type}/'\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "        \n",
    "    with open(f\"{output}/X.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(X_all, fp)\n",
    "    with open(f\"{output}/y.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(y_all, fp)\n",
    "\n",
    "        \n",
    "        \n",
    "def xDawn_denosing_participants(data, n_components=2, verbose=False, **kwargs):\n",
    "    if not verbose:\n",
    "        cm = HiddenPrints()\n",
    "    else:\n",
    "        cm = nullcontext()\n",
    "    with cm:\n",
    "        data.events = mne.merge_events(data.events, [1,2,3,4,5,6,7,8,9], 1, replace_events=True)\n",
    "        data.event_id = {'Word': 1, 'Pseudo': 10}\n",
    "        temp_syg = data.get_data()\n",
    "        temp_syg = temp_syg.reshape([temp_syg.shape[1],temp_syg.shape[0]*temp_syg.shape[2]]) #ERROR\n",
    "        signal_cov = mne.compute_raw_covariance(mne.io.RawArray(temp_syg, data[0].info), verbose=verbose)\n",
    "        xd = mne.preprocessing.Xdawn(n_components=n_components, signal_cov=signal_cov)\n",
    "        xd.fit(data)\n",
    "        epochs_denoised = xd.apply(data)\n",
    "        data = next(iter(epochs_denoised.values())) #Potencjal wywyolany 200ms gorka\n",
    "    return data\n",
    "    \n",
    "\n",
    "def standardization_participants(data, **kwargs):\n",
    "    data_temp = data.get_data()\n",
    "    mean = np.mean(data_temp, axis=2).reshape(data_temp.shape[0], data_temp.shape[1], 1)\n",
    "    std = np.std(data_temp, axis=(1, 2)).reshape(data_temp.shape[0], 1, 1)\n",
    "    data._data = (data_temp - mean)/std\n",
    "    return data\n",
    "\n",
    "\n",
    "def standardizatin_common_participants(data, **kwargs):\n",
    "    data_temp = data.get_data()\n",
    "    mean = np.mean(np.mean(data_temp, axis=2).reshape(data_temp.shape[0], data_temp.shape[1], 1),axis=0)\n",
    "    std = np.mean(np.mean(np.std(data_temp, axis=(1, 2)).reshape(data_temp.shape[0], 1, 1)))\n",
    "    data._data = (data_temp - mean)/std\n",
    "    return data\n",
    "\n",
    "def split_participants(data_type = '', output = './processed_data/'):\n",
    "    with open(f'./{output}/{data_type}/X.pkl', 'rb') as f:\n",
    "        X_all = pickle.load(f)\n",
    "    with open(f'./{output}/{data_type}/y.pkl', 'rb') as f:\n",
    "        y_all = pickle.load(f)\n",
    "    \n",
    "    X_test_all = np.zeros((0, 19, 306))\n",
    "    X_train_all = np.zeros((0, 19, 306))\n",
    "    X_valid_all = np.zeros((0, 19, 306))\n",
    "\n",
    "    y_test_all = np.zeros((0))\n",
    "    y_train_all = np.zeros((0))\n",
    "    y_valid_all = np.zeros((0))\n",
    "\n",
    "    for i in range(0, 20):\n",
    "        X_train_all = np.concatenate((X_train_all, X_all[i]), axis = 0)\n",
    "        y_train_all = np.concatenate((y_train_all, y_all[i]), axis = 0)\n",
    "\n",
    "    for i in range(20, 25):\n",
    "        X_test_all = np.concatenate((X_test_all, X_all[i]), axis = 0)\n",
    "        y_test_all = np.concatenate((y_test_all, y_all[i]), axis = 0)\n",
    "\n",
    "    for i in range(25, 30):\n",
    "        X_valid_all = np.concatenate((X_valid_all, X_all[i]), axis = 0)\n",
    "        y_valid_all = np.concatenate((y_valid_all, y_all[i]), axis = 0)\n",
    "                       \n",
    "    output = f'{output}/{data_type}/'\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "        \n",
    "    np.save(output + f'X_test.npy', X_test_all)\n",
    "    np.save(output + f'y_test.npy', y_test_all)\n",
    "    np.save(output + f'X_train.npy', X_train_all)\n",
    "    np.save(output + f'y_train.npy', y_train_all)\n",
    "    np.save(output + f'X_valid.npy', X_valid_all)\n",
    "    np.save(output + f'y_valid.npy', y_valid_all)\n",
    "    \n",
    "def split_participants_folds(data_type = '', output = './processed_data/', num_folds=5):\n",
    "    with open(f'./{output}/{data_type}/X.pkl', 'rb') as f:\n",
    "        X_all = pickle.load(f)\n",
    "    with open(f'./{output}/{data_type}/y.pkl', 'rb') as f:\n",
    "        y_all = pickle.load(f)\n",
    "\n",
    "    X_test_all = np.zeros((0, 19, 306))\n",
    "    X_train_all = np.zeros((0, 19, 306))\n",
    "    X_valid_all = np.zeros((0, 19, 306))\n",
    "\n",
    "    y_test_all = np.zeros((0))\n",
    "    y_train_all = np.zeros((0))\n",
    "    y_valid_all = np.zeros((0))\n",
    "\n",
    "    output = f'{output}/{data_type}/folds/'\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "\n",
    "    for i in range(25, 30):\n",
    "        X_valid_all = np.concatenate((X_valid_all, X_all[i]), axis = 0)\n",
    "        y_valid_all = np.concatenate((y_valid_all, y_all[i]), axis = 0)\n",
    "\n",
    "    np.save(output + f'X_valid.npy', X_valid_all)\n",
    "    np.save(output + f'y_valid.npy', y_valid_all)\n",
    "\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "    fold = 0\n",
    "    for train, test in kfold.split(X_all[:25], y_all[:25]):\n",
    "        X_test = np.zeros((0, 19, 306))\n",
    "        X_train = np.zeros((0, 19, 306))\n",
    "        y_test = np.zeros((0))\n",
    "        y_train = np.zeros((0))\n",
    "\n",
    "        for i in train:\n",
    "            X_train = np.concatenate((X_train, X_all[i]), axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_all[i]), axis = 0)\n",
    "        for i in test:\n",
    "            X_test = np.concatenate((X_test, X_all[i]), axis = 0)\n",
    "            y_test = np.concatenate((y_test, y_all[i]), axis = 0)\n",
    "\n",
    "        np.save(output + f'X_test_fold_{fold}.npy', X_test)\n",
    "        np.save(output + f'y_test_fold_{fold}.npy', y_test)\n",
    "        np.save(output + f'X_train_fold_{fold}.npy', X_train)\n",
    "        np.save(output + f'y_train_fold_{fold}.npy', y_train)\n",
    "        fold +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing clean participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing B01\n",
      "Importing B02\n",
      "Importing B03\n",
      "Importing B04\n",
      "Importing B05\n",
      "Importing B06\n",
      "Importing B07\n",
      "Importing B08\n",
      "Importing B09\n",
      "Importing B10\n",
      "Importing B11\n",
      "Importing B15\n",
      "Importing B17\n",
      "Importing B18\n",
      "Importing B20\n",
      "Importing B21\n",
      "Importing B22\n",
      "Importing B24\n",
      "Importing B26\n",
      "Importing B27\n",
      "Importing B28\n",
      "Importing B29\n",
      "Importing B30\n",
      "Importing B31\n",
      "Importing B32\n",
      "Importing B33\n",
      "Importing B34\n",
      "Importing B35\n",
      "Importing B36\n",
      "Importing B37\n"
     ]
    }
   ],
   "source": [
    "data_processing_participants(data_type = 'clean_participants', functions=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_participants(data_type = 'clean_participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_participants_folds(data_type = 'clean_participants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing xDawn participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing B01\n",
      "Importing B02\n",
      "Importing B03\n",
      "Importing B04\n",
      "Importing B05\n",
      "Importing B06\n",
      "Importing B07\n",
      "Importing B08\n",
      "Importing B09\n",
      "Importing B10\n",
      "Importing B11\n",
      "Importing B15\n",
      "Importing B17\n",
      "Importing B18\n",
      "Importing B20\n",
      "Importing B21\n",
      "Importing B22\n",
      "Importing B24\n",
      "Importing B26\n",
      "Importing B27\n",
      "Importing B28\n",
      "Importing B29\n",
      "Importing B30\n",
      "Importing B31\n",
      "Importing B32\n",
      "Importing B33\n",
      "Importing B34\n",
      "Importing B35\n",
      "Importing B36\n",
      "Importing B37\n"
     ]
    }
   ],
   "source": [
    "data_processing_participants(data_type = 'xDawn_participants', functions=[xDawn_denosing_participants])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_participants(data_type = 'xDawn_participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_participants_folds(data_type = 'xDawn_participants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing standardization common participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing B01\n",
      "Importing B02\n",
      "Importing B03\n",
      "Importing B04\n",
      "Importing B05\n",
      "Importing B06\n",
      "Importing B07\n",
      "Importing B08\n",
      "Importing B09\n",
      "Importing B10\n",
      "Importing B11\n",
      "Importing B15\n",
      "Importing B17\n",
      "Importing B18\n",
      "Importing B20\n",
      "Importing B21\n",
      "Importing B22\n",
      "Importing B24\n",
      "Importing B26\n",
      "Importing B27\n",
      "Importing B28\n",
      "Importing B29\n",
      "Importing B30\n",
      "Importing B31\n",
      "Importing B32\n",
      "Importing B33\n",
      "Importing B34\n",
      "Importing B35\n",
      "Importing B36\n",
      "Importing B37\n"
     ]
    }
   ],
   "source": [
    "data_processing_participants(data_type = 'standardization_common_participants', functions=[standardizatin_common_participants])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_participants(data_type = 'standardization_common_participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_participants_folds(data_type = 'standardization_common_participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_braindecode",
   "language": "python",
   "name": "new_braindecode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
